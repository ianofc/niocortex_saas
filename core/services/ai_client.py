import os
import google.generativeai as genai
from django.conf import settings
import traceback
import time

# Configura√ß√£o da API Key
api_key = os.getenv("GOOGLE_API_KEY")
if api_key:
    genai.configure(api_key=api_key)

def get_ai_response(messages, temperature=0.7):
    """
    Wrapper para o Google Gemini focado no modelo 2.5-flash (Dispon√≠vel na sua conta).
    """
    if not api_key:
        return "‚ö†Ô∏è ERRO CONFIG: GOOGLE_API_KEY n√£o encontrada."

    # Prepara o hist√≥rico
    chat_history = []
    last_user_message = ""
    system_instruction = None

    try:
        # 1. Separa System, History e Mensagem Atual
        for msg in messages:
            role = msg.get('role')
            content = msg.get('content', '')
            
            if not content and role != 'model': continue

            if role == 'system':
                system_instruction = content
            elif role == 'user':
                chat_history.append({"role": "user", "parts": [content]})
                last_user_message = content
            elif role in ['assistant', 'model']:
                chat_history.append({"role": "model", "parts": [content]})

        # Remove a √∫ltima mensagem do usu√°rio do hist√≥rico (pois ela ser√° o prompt atual)
        if chat_history and chat_history[-1]['role'] == 'user':
            chat_history.pop()
        
        if not last_user_message:
            return "N√£o entendi. Pode repetir?"

        # 2. Defini√ß√£o do Modelo (2.5 Confirmado pelo seu log)
        model_name = "gemini-2.5-flash"
        
        print(f"ü§ñ IA: Iniciando chat com {model_name}...")

        model = genai.GenerativeModel(
            model_name=model_name,
            system_instruction=system_instruction,
            generation_config={"temperature": temperature}
        )

        chat = model.start_chat(history=chat_history)
        response = chat.send_message(last_user_message)
        return response.text

    except Exception as e:
        error_msg = str(e)
        
        # Tratamento espec√≠fico para Cota Excedida (429)
        if "429" in error_msg or "Quota exceeded" in error_msg:
            print("‚è≥ Cota excedida. Aguardando 2s para tentar fallback...")
            time.sleep(2)
            return "Minha capacidade de processamento est√° sobrecarregada no momento. Tente novamente em alguns segundos."

        # Log gen√©rico
        print("\n" + "="*50)
        print("‚ùå ERRO NO GEMINI AI:")
        traceback.print_exc()
        print("="*50 + "\n")
        
        return f"‚ö†Ô∏è Erro t√©cnico na IA: {error_msg}"

# CLASSE LEGADA (Mantida para compatibilidade)
class AIClient:
    def __init__(self):
        self.api_key = api_key

    @staticmethod
    def check_proactive_thought(user, path, meta):
        """Simula√ß√£o para evitar erro na view api_check_zios"""
        return {"should_speak": False}

    @staticmethod
    def chat_universal(message, user):
        """Wrapper para o chat universal"""
        return {"reply": get_ai_response([{'role': 'user', 'content': message}])}

    def ask(self, prompt, context=None):
        messages = []
        if context:
            messages.append({'role': 'system', 'content': context})
        messages.append({'role': 'user', 'content': prompt})
        return get_ai_response(messages)

    def generate_content(self, prompt):
        return get_ai_response([{'role': 'user', 'content': prompt}])